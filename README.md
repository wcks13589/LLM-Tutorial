# LLM Bootcamp - NVIDIA NeMo å¤§å‹èªè¨€æ¨¡å‹è¨“ç·´å¯¦æˆ°æ•™å­¸ ğŸš€

æ­¡è¿ä¾†åˆ° LLM Bootcampï¼æœ¬æ•™å­¸å°‡å¸¶æ‚¨å®Œæ•´é«”é©—ä½¿ç”¨ [NVIDIA NeMo](https://github.com/NVIDIA/NeMo) é€²è¡Œå¤§å‹èªè¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å®Œæ•´æµç¨‹ï¼Œå¾é›¶é–‹å§‹å­¸æœƒæ¨¡å‹è½‰æ›ã€é è¨“ç·´ã€å¾®èª¿åˆ°éƒ¨ç½²çš„å¯¦æˆ°æŠ€å·§ã€‚

## ğŸ¯ å­¸ç¿’ç›®æ¨™

é€šéæœ¬ Bootcampï¼Œæ‚¨å°‡å­¸æœƒï¼š

1. **ğŸ”„ æ¨¡å‹è½‰æ›æŠ€èƒ½**ï¼šæŒæ¡ Hugging Face èˆ‡ NeMo æ ¼å¼é–“çš„è½‰æ›
2. **ğŸ“š é è¨“ç·´å¯¦è¸**ï¼šé«”é©—å¤§è¦æ¨¡èªè¨€æ¨¡å‹çš„æŒçºŒé è¨“ç·´
3. **ğŸ› ï¸ å¾®èª¿æŠ€è¡“**ï¼šå­¸æœƒé‡å°ç‰¹å®šä»»å‹™é€²è¡Œæ¨¡å‹å¾®èª¿ã€æŒæ¡ LoRA ç­‰åƒæ•¸é«˜æ•ˆå¾®èª¿æ–¹æ³•
4. **ğŸ“Š æ¨¡å‹è©•ä¼°**ï¼šå­¸æœƒè©•ä¼°å’Œæ¸¬è©¦æ¨¡å‹æ€§èƒ½
5. **ğŸš€ æ¨¡å‹éƒ¨ç½²**ï¼šäº†è§£æ¨¡å‹å°å‡ºå’Œéƒ¨ç½²æµç¨‹

---

## ğŸ“‚ æ•™å­¸å¤§ç¶±

- [ğŸš€ é–‹å§‹ä¹‹å‰ï¼šç’°å¢ƒè¨­å®š](#ğŸ› ï¸-ç’°å¢ƒæº–å‚™)
  - [ğŸ“– è©³ç´°ç’°å¢ƒè¨­å®šæŒ‡å—](setup/README.md) â­
- [ğŸ“¥ å°ˆæ¡ˆè¨­ç½®](#ğŸ“¥-å°ˆæ¡ˆè¨­ç½®)
- [ğŸ“– è©³ç´°æ•™å­¸æ­¥é©Ÿ](#ğŸ“–-è©³ç´°æ•™å­¸æ­¥é©Ÿ)
  - [ç¬¬ä¸€ç« ï¼šæ¨¡å‹è½‰æ›åŸºç¤](#ç¬¬ä¸€ç« æ¨¡å‹è½‰æ›åŸºç¤)
  - [ç¬¬äºŒç« ï¼šæŒçºŒé è¨“ç·´å¯¦æˆ°](#ç¬¬äºŒç« æŒçºŒé è¨“ç·´å¯¦æˆ°)
  - [ç¬¬ä¸‰ç« ï¼šæŒ‡ä»¤å¾®èª¿æŠ€è¡“](#ç¬¬ä¸‰ç« æŒ‡ä»¤å¾®èª¿æŠ€è¡“)
  - [ç¬¬å››ç« ï¼šReasoning è³‡æ–™å¾®èª¿æŠ€è¡“](#ç¬¬å››ç« reasoning-è³‡æ–™å¾®èª¿æŠ€è¡“)
  - [ç¬¬äº”ç« ï¼šæ¨¡å‹è©•ä¼°èˆ‡æ¸¬è©¦](#ç¬¬äº”ç« æ¨¡å‹è©•ä¼°èˆ‡æ¸¬è©¦)
  - [ç¬¬å…­ç« ï¼šæ¨¡å‹éƒ¨ç½²èˆ‡è½‰æ›](#ç¬¬å…­ç« æ¨¡å‹éƒ¨ç½²èˆ‡è½‰æ›)
- [ğŸ’¡ å¯¦æˆ°æŠ€å·§](#ğŸ’¡-å¯¦æˆ°æŠ€å·§)
- [ğŸ“š é€²éšå­¸ç¿’è³‡æº](#ğŸ“š-é€²éšå­¸ç¿’è³‡æº)

---

## ğŸ› ï¸ ç’°å¢ƒæº–å‚™

### ğŸ“‹ é¸æ“‡æ‚¨çš„ç’°å¢ƒ

åœ¨é–‹å§‹è¨“ç·´ä¹‹å‰ï¼Œæ‚¨éœ€è¦è¨­å®šé©åˆçš„ GPU ç’°å¢ƒã€‚æˆ‘å€‘æä¾›è©³ç´°çš„ç’°å¢ƒè¨­å®šæŒ‡å—ï¼š

#### ğŸŒŸ TWCC é›²ç«¯ç’°å¢ƒ

**è¨­å®šæ­¥é©Ÿ**ï¼šè©³è¦‹ **[ğŸ“– TWCC ç’°å¢ƒè¨­å®šæŒ‡å—](setup/README.md)**

#### ğŸ³ æœ¬åœ° Docker ç’°å¢ƒ

ä½¿ç”¨å®˜æ–¹ NeMo å®¹å™¨ï¼ŒåŒ…å«æ‰€æœ‰å¿…è¦çš„ä¾è³´å¥—ä»¶ï¼š

```bash
docker run \
    --gpus all -it --rm --shm-size=8g --ulimit memlock=-1 --ulimit stack=67108864 \
    -v $PWD:$PWD -w $PWD -p 8888:8888 \
    nvcr.io/nvidia/nemo:25.04
```

> ğŸ’¡ **æç¤º**ï¼šæ‚¨å¯ä»¥åœ¨ [NGC NeMo Catalog](https://catalog.ngc.nvidia.com/orgs/nvidia/containers/nemo/tags) æŸ¥çœ‹æœ€æ–°ç‰ˆæœ¬

---

## ğŸ“¥ å°ˆæ¡ˆè¨­ç½®

### ä¸‹è¼‰æ•™å­¸å°ˆæ¡ˆ

```bash
git clone https://github.com/wcks13589/LLM-Tutorial.git
cd LLM-Tutorial
```

> ğŸ’¡ **æç¤º**ï¼šè«‹ç¢ºä¿åœ¨ `LLM-Tutorial` å°ˆæ¡ˆç›®éŒ„ä¸­åŸ·è¡Œå¾ŒçºŒæŒ‡ä»¤ã€‚

### ğŸ”‘ è¨­å®š Hugging Face æ¬Šé™

ç”³è«‹ä¸¦è¨­å®šæ‚¨çš„ Hugging Face Tokenï¼š

1. **ç”³è«‹ Token**ï¼šå‰å¾€ [Hugging Face Settings](https://huggingface.co/settings/tokens) å»ºç«‹æ–°çš„ Access Token
2. **è¨­å®šç’°å¢ƒè®Šæ•¸**ï¼š
   ```bash
   # æ›¿æ›ç‚ºæ‚¨çš„å¯¦éš› Token
   export HF_TOKEN="your_hf_token"
   huggingface-cli login --token $HF_TOKEN
   ```

> ğŸ“Œ **é‡è¦**ï¼šè«‹å…ˆåœ¨ [Hugging Face](https://huggingface.co/settings/tokens) ç”³è«‹ Access Token

## ğŸ“– è©³ç´°æ•™å­¸æ­¥é©Ÿ

### ç¬¬ä¸€ç« ï¼šæ¨¡å‹è½‰æ›åŸºç¤

#### 1.1 ä¸‹è¼‰é è¨“ç·´æ¨¡å‹

```bash
# ä¸‹è¼‰ Llama 3.1 8B Instruct æ¨¡å‹
huggingface-cli download meta-llama/Llama-3.1-8B-Instruct \
    --local-dir Llama-3.1-8B-Instruct \
    --exclude original/
```

#### 1.2 è½‰æ›ç‚º NeMo æ ¼å¼

```bash
# è¨­å®šè®Šæ•¸
MODEL=llama3_8b
HF_MODEL_ID=Llama-3.1-8B-Instruct
OUTPUT_PATH=nemo_ckpt/Llama-3.1-8B-Instruct
OVERWRITE_EXISTING=false

# åŸ·è¡Œè½‰æ›
nemo llm import -y \
    model=${MODEL} \
    source=hf://${HF_MODEL_ID} \
    output_path=${OUTPUT_PATH} \
    overwrite=${OVERWRITE_EXISTING}
```

> âœ… **æª¢æŸ¥é»**ï¼šç¢ºèª `nemo_ckpt/` ç›®éŒ„ä¸‹æˆåŠŸç”Ÿæˆäº† NeMo æ ¼å¼çš„æ¨¡å‹æª”æ¡ˆ

---

### ç¬¬äºŒç« ï¼šæŒçºŒé è¨“ç·´å¯¦æˆ°

#### 2.1 æº–å‚™è¨“ç·´è³‡æ–™

**ä¸‹è¼‰ä¸­æ–‡è³‡æ–™é›†**ï¼š

```bash
python data_preparation/download_pretrain_data.py \
    --dataset_name erhwenkuo/wikinews-zhtw \
    --output_dir data/custom_dataset/json/wikinews-zhtw.jsonl
```

#### 2.2 è³‡æ–™é è™•ç†

```bash
# å»ºç«‹é è™•ç†ç›®éŒ„
mkdir -p data/custom_dataset/preprocessed

# ä½¿ç”¨ NeMo çš„è³‡æ–™é è™•ç†å·¥å…·
python /opt/NeMo/scripts/nlp_language_modeling/preprocess_data_for_megatron.py \
    --input=data/custom_dataset/json/wikinews-zhtw.jsonl \
    --json-keys=text \
    --dataset-impl mmap \
    --tokenizer-library=huggingface \
    --tokenizer-type meta-llama/Llama-3.1-8B-Instruct \
    --output-prefix=data/custom_dataset/preprocessed/wikinews \
    --append-eod
```

#### 2.3 åŸ·è¡Œé è¨“ç·´

##### å‰ç½®æº–å‚™

è¨­å®šåŸºæœ¬åƒæ•¸ï¼š

```bash
JOB_NAME=llama31_pretraining
NUM_NODES=1
NUM_GPUS=1
HF_MODEL_ID=meta-llama/Llama-3.1-8B-Instruct

# ä¸¦è¡Œè™•ç†åƒæ•¸
TP=1  # Tensor Parallel
PP=1  # Pipeline Parallel  
CP=1  # Context Parallel

# è¨“ç·´åƒæ•¸
GBS=8          # Global Batch Size
MAX_STEPS=20  # æœ€å¤§è¨“ç·´æ­¥æ•¸(æ¨¡å‹æ¬Šé‡æ›´æ–°æ¬¡æ•¸)
DATASET_PATH=data/custom_dataset/preprocessed/
```

##### æ–¹æ³•ä¸€ï¼šå¾é ­é–‹å§‹é è¨“ç·´æ¨¡å‹

**é©ç”¨æƒ…æ³**ï¼šç•¶æ‚¨æƒ³è¦å¾é›¶é–‹å§‹è¨“ç·´æ¨¡å‹æ™‚ä½¿ç”¨ã€‚

**ç‰¹é»**ï¼šè…³æœ¬æœƒè‡ªå‹•å¾åŸºç¤æ¨¡å‹æ¶æ§‹é€²è¡Œæ¬Šé‡åˆå§‹åŒ–

**åŸ·è¡ŒæŒ‡ä»¤**ï¼š
```bash
python pretraining/pretrain.py \
   --executor local \
   --experiment ${JOB_NAME} \
   --num_nodes ${NUM_NODES} \
   --num_gpus ${NUM_GPUS} \
   --model_size 8B \
   --hf_model_id ${HF_MODEL_ID} \
   --hf_token ${HF_TOKEN} \
   --max_steps ${MAX_STEPS} \
   --global_batch_size ${GBS} \
   --tensor_model_parallel_size ${TP} \
   --pipeline_model_parallel_size ${PP} \
   --context_parallel_size ${CP} \
   --dataset_path ${DATASET_PATH}
```

> **é‡è¦æé†’**ï¼šæœ¬æ•™å­¸å…§å®¹ç‰¹åˆ¥é‡å° V100 32GB GPU é€²è¡Œé…ç½®å„ªåŒ–
> 
> ç”±æ–¼æœ¬æ•™å­¸å…§å®¹é è¨ˆä½¿ç”¨ V100 32GB çš„ GPU ä¾†å¯¦ä½œï¼Œç‚ºç¢ºä¿å¯ä»¥é †åˆ©åŸ·è¡Œæ¨¡å‹çš„è¨“ç·´ï¼Œæˆ‘å€‘åœ¨ `pretrain.py` ä¸­ç‰¹åœ°å°‡æ¨¡å‹çš„å±¤æ•¸èˆ‡ç¶­åº¦å¤§å¹…é™ä½ï¼š
> 
> **æ¨¡å‹é…ç½®å°æ¯”**ï¼š
> - **åŸå§‹ Llama3.1 8B æ¨¡å‹**ï¼š
>   - `num_layers = 32`
>   - `hidden_size = 4096`
>   - åƒæ•¸é‡ï¼š 8B å€‹åƒæ•¸
> - **èª¿æ•´å¾Œé…ç½®**ï¼š
>   - `num_layers = 1`
>   - `hidden_size = 128`
>   - åƒæ•¸é‡ï¼šå¤§å¹…é™ä½ï¼Œé©åˆå–®å¼µ V100 GPU
> 
> **èª¿æ•´åŸå› **ï¼š
> - ç¢ºä¿åœ¨ V100 32GB è¨˜æ†¶é«”é™åˆ¶ä¸‹èƒ½é †åˆ©åŸ·è¡Œ
> - é™ä½è¨“ç·´æ™‚é–“ï¼Œæä¾›æ›´å¥½çš„å­¸ç¿’é«”é©—
> - ä¿æŒå®Œæ•´çš„è¨“ç·´æµç¨‹ï¼Œè®“å­¸ç¿’è€…ç†è§£æ•´å€‹é è¨“ç·´éç¨‹
> 
> **ç¨‹å¼ç¢¼ä½ç½®**ï¼šé€™äº›é…ç½®èª¿æ•´ä½æ–¼ `pretrain.py` ä¸­çš„ `configure_recipe` å‡½æ•¸å…§ã€‚
> 
> **å…·é«”ä¿®æ”¹çš„ç¨‹å¼ç¢¼**ï¼š
> ```python
> recipe.model.config.num_layers = 1
> recipe.model.config.hidden_size = 128
> ```

> ğŸ“Š **ç›£æ§è¨“ç·´**ï¼šè¨“ç·´éç¨‹ä¸­å¯ä»¥è§€å¯Ÿ loss è®ŠåŒ–ä¾†åˆ¤æ–·æ¨¡å‹å­¸ç¿’ç‹€æ³

##### æ–¹æ³•äºŒï¼šå¾é è¨“ç·´æ¨¡å‹é–‹å§‹ç¹¼çºŒé è¨“ç·´

**é©ç”¨æƒ…æ³**ï¼šç•¶æ‚¨æƒ³è¦å¾ç¾æœ‰çš„ NeMo æ ¼å¼æ¨¡å‹é–‹å§‹ï¼Œé€²è¡ŒæŒçºŒé è¨“ç·´æ™‚ä½¿ç”¨ã€‚

**å‰ç½®æ¢ä»¶**ï¼š
- éœ€è¦å…ˆå°‡ Hugging Face æ¨¡å‹è½‰æ›ç‚º NeMo æ ¼å¼
- ç¢ºä¿ `${NEMO_MODEL}` è·¯å¾‘ä¸‹å­˜åœ¨æœ‰æ•ˆçš„ NeMo æ¨¡å‹æª”æ¡ˆ

**åŸ·è¡ŒæŒ‡ä»¤**ï¼š
```bash
NEMO_MODEL=nemo_ckpt/Llama-3.1-8B-Instruct

python pretraining/pretrain.py \
   --executor local \
   --experiment ${JOB_NAME} \
   --num_nodes ${NUM_NODES} \
   --num_gpus ${NUM_GPUS} \
   --model_size 8B \
   --hf_model_id ${HF_MODEL_ID} \
   --nemo_model ${NEMO_MODEL} \
   --hf_token ${HF_TOKEN} \
   --max_steps ${MAX_STEPS} \
   --global_batch_size ${GBS} \
   --tensor_model_parallel_size ${TP} \
   --pipeline_model_parallel_size ${PP} \
   --context_parallel_size ${CP} \
   --dataset_path ${DATASET_PATH}
```

---

### ç¬¬ä¸‰ç« ï¼šæŒ‡ä»¤å¾®èª¿æŠ€è¡“

#### 3.1 æº–å‚™å¾®èª¿è³‡æ–™

```bash
# ä¸‹è¼‰ä¸¦æº–å‚™ Alpaca è³‡æ–™é›†
python data_preparation/download_sft_data.py
```

#### 3.2 åŸ·è¡Œ LoRA å¾®èª¿

```bash
# å¾®èª¿åƒæ•¸è¨­å®š
JOB_NAME=llama31_finetuning
NUM_NODES=1
NUM_GPUS=1
HF_MODEL_ID=meta-llama/Llama-3.1-8B-Instruct
NEMO_MODEL=nemo_ckpt/Llama-3.1-8B-Instruct
# LATEST_CHECKPOINT=$(find nemo_experiments/llama31_pretraining/checkpoints/ -type d -name "*-last" | sort -r | head -n 1)
HF_TOKEN=$HF_TOKEN

# ä¸¦è¡Œè™•ç†åƒæ•¸
TP=1
PP=1
CP=1

# å¾®èª¿åƒæ•¸
MAX_STEPS=20
GBS=8
DATASET_PATH=data/alpaca

# åŸ·è¡Œ LoRA å¾®èª¿
python finetuning/finetune.py \
    --executor local \
    --experiment ${JOB_NAME} \
    --num_nodes ${NUM_NODES} \
    --num_gpus ${NUM_GPUS} \
    --model_size 8B \
    --hf_model_id ${HF_MODEL_ID} \
    --hf_token ${HF_TOKEN} \
    --nemo_model ${NEMO_MODEL} \
    --max_steps ${MAX_STEPS} \
    --global_batch_size ${GBS} \
    --tensor_model_parallel_size ${TP} \
    --pipeline_model_parallel_size ${PP} \
    --context_parallel_size ${CP} \
    --dataset_path ${DATASET_PATH} \
    --peft lora
```

> ğŸ¯ **å…¨åƒæ•¸å¾®èª¿**ï¼šè‹¥è¦é€²è¡Œå…¨åƒæ•¸çš„å¾®èª¿ï¼Œè«‹ç§»é™¤`--peft lora`

---

### ç¬¬å››ç« ï¼šReasoning è³‡æ–™å¾®èª¿æŠ€è¡“

#### 4.1 æº–å‚™ Reasoning è³‡æ–™é›†

```bash
# å»ºç«‹ Reasoning è³‡æ–™é›†ç›®éŒ„
mkdir -p data/reasoning_dataset/

# ä¸‹è¼‰ NVIDIA Llama-Nemotron å¾Œè¨“ç·´è³‡æ–™é›†
wget https://huggingface.co/datasets/nvidia/Llama-Nemotron-Post-Training-Dataset/resolve/main/SFT/chat/chat.jsonl -P data/reasoning_dataset/
```

#### 4.2 è³‡æ–™é è™•ç†èˆ‡ç­–å±•

```bash
# åŸ·è¡Œè³‡æ–™ç­–å±•èˆ‡é è™•ç†
python data_preparation/curate_reasoning_data.py \
    --input-dir "data/reasoning_dataset" \
    --filename-filter "chat" \
    --remove-columns "category" "generator" "license" "reasoning" "system_prompt" "used_in_training" "version" \
    --json-files-per-partition 16 \
    --tokenizer "meta-llama/Llama-3.1-8B-Instruct" \
    --max-token-count 16384 \
    --max-completion-token-count 8192 \
    --output-dir data/reasoning_dataset/curated-data \
    --device "gpu" \
    --n-workers 1
```

#### 4.3 åŸ·è¡Œ Reasoning LoRA å¾®èª¿

```bash
# Reasoning å¾®èª¿åƒæ•¸è¨­å®š
JOB_NAME=llama31_reasoning_finetuning
NUM_NODES=1
NUM_GPUS=1
HF_MODEL_ID=meta-llama/Llama-3.1-8B-Instruct
NEMO_MODEL=nemo_ckpt/Llama-3.1-8B-Instruct
HF_TOKEN=$HF_TOKEN

# ä¸¦è¡Œè™•ç†åƒæ•¸
TP=1
PP=1
CP=1

# å¾®èª¿åƒæ•¸
MAX_STEPS=20
GBS=8
DATASET_PATH=data/reasoning_dataset/curated-data

# åŸ·è¡Œ Reasoning LoRA å¾®èª¿
python finetuning/finetune.py \
    --executor local \
    --experiment ${JOB_NAME} \
    --num_nodes ${NUM_NODES} \
    --num_gpus ${NUM_GPUS} \
    --model_size 8B \
    --hf_model_id ${HF_MODEL_ID} \
    --hf_token ${HF_TOKEN} \
    --nemo_model ${NEMO_MODEL} \
    --max_steps ${MAX_STEPS} \
    --global_batch_size ${GBS} \
    --tensor_model_parallel_size ${TP} \
    --pipeline_model_parallel_size ${PP} \
    --context_parallel_size ${CP} \
    --dataset_path ${DATASET_PATH} \
    --peft lora
```

> ğŸ§  **Reasoning å¾®èª¿ç‰¹è‰²**ï¼šä½¿ç”¨é«˜å“è³ªçš„æ¨ç†è³‡æ–™é›†ï¼Œæå‡æ¨¡å‹çš„é‚è¼¯æ¨ç†å’Œè¤‡é›œå•é¡Œè§£æ±ºèƒ½åŠ›

---

### ç¬¬äº”ç« ï¼šæ¨¡å‹è©•ä¼°èˆ‡æ¸¬è©¦

#### 5.1 æº–å‚™æ¸¬è©¦è³‡æ–™

```bash
# å¾æ¸¬è©¦é›†ä¸­é¸å–æ¨£æœ¬é€²è¡Œå¿«é€Ÿè©•ä¼°
head -n 30 data/alpaca/test.jsonl > data/alpaca/test_subset.jsonl
```

#### 5.2 åŸ·è¡Œæ¨ç†æ¸¬è©¦

```bash
# ä½¿ç”¨å¾®èª¿å¾Œçš„æ¨¡å‹é€²è¡Œæ¨ç†
# æ‰¾åˆ°æœ€æ–°çš„æª¢æŸ¥é»è³‡æ–™å¤¾
LATEST_CHECKPOINT=$(find nemo_experiments/llama31_finetuning/checkpoints/ -type d -name "*-last" | sort -r | head -n 1)

python evaluation/inference.py \
    --peft_ckpt_path ${LATEST_CHECKPOINT} \
    --input_dataset data/alpaca/test_subset.jsonl \
    --output_path data/alpaca/peft_prediction.jsonl
```

#### 5.3 è¨ˆç®—è©•ä¼°æŒ‡æ¨™

```bash
# è¨ˆç®—æ¨¡å‹æ€§èƒ½æŒ‡æ¨™
python /opt/NeMo/scripts/metric_calculation/peft_metric_calc.py \
    --pred_file data/alpaca/peft_prediction.jsonl \
    --label_field "label" \
    --pred_field "prediction"
```

---

### ç¬¬å…­ç« ï¼šæ¨¡å‹éƒ¨ç½²èˆ‡è½‰æ›

#### 6.1 è½‰æ›å› Hugging Face æ ¼å¼

```bash
# è¨­å®šè½‰æ›åƒæ•¸
OUTPUT_PATH=hf_ckpt

# åŸ·è¡Œè½‰æ›
nemo llm export -y \
    path=${LATEST_CHECKPOINT} \
    output_path=${OUTPUT_PATH} \
    target=hf
```

## ğŸ“š é€²éšå­¸ç¿’è³‡æº

### å®˜æ–¹æ–‡æª”
- [NeMo å®˜æ–¹æ–‡ä»¶](https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/stable/)
- [NeMo GitHub](https://github.com/NVIDIA/NeMo)

### é€²éšä¸»é¡Œ
1. **å¤šæ¨¡æ…‹æ¨¡å‹è¨“ç·´**
2. **åˆ†æ•£å¼è¨“ç·´å„ªåŒ–**
3. **æ¨¡å‹å£“ç¸®èˆ‡é‡åŒ–**
4. **è‡ªå®šç¾©è³‡æ–™è¼‰å…¥å™¨**

---

## ğŸ‰ æ­å–œå®Œæˆ LLM Bootcampï¼

é€šéæœ¬æ•™å­¸ï¼Œæ‚¨å·²ç¶“æŒæ¡äº†ï¼š
- âœ… å¤§å‹èªè¨€æ¨¡å‹çš„å®Œæ•´è¨“ç·´æµç¨‹
- âœ… NeMo æ¡†æ¶çš„æ ¸å¿ƒåŠŸèƒ½
- âœ… å¯¦éš›çš„ AI æ¨¡å‹é–‹ç™¼æŠ€èƒ½
- âœ… ä¼æ¥­ç´š AI æ‡‰ç”¨é–‹ç™¼åŸºç¤

**ä¸‹ä¸€æ­¥å»ºè­°**ï¼š
1. å˜—è©¦ä½¿ç”¨è‡ªå·±çš„è³‡æ–™é›†
2. æ¢ç´¢ä¸åŒçš„æ¨¡å‹æ¶æ§‹
3. å­¸ç¿’æ¨¡å‹éƒ¨ç½²èˆ‡æœå‹™åŒ–
4. åƒèˆ‡é–‹æºå°ˆæ¡ˆè²¢ç»

---

> ğŸ’¬ **éœ€è¦å¹«åŠ©ï¼Ÿ** æ­¡è¿åœ¨ [Issues](https://github.com/wcks13589/NeMo-Tutorial/issues) ä¸­æå‡ºå•é¡Œæˆ–å»ºè­°ï¼

**Happy Learning! ğŸš€**